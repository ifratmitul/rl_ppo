{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wfdb --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport wfdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\n\n\n# Define dataset path\ndata_path = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/\"\n\n# Load SCP-ECG Statements (Error Handling Applied)\ntry:\n    scp_df = pd.read_csv(data_path + \"scp_statements.csv\", encoding='utf-8')\nexcept Exception as e:\n    print(\"Error loading scp_statements.csv:\", e)\n\n# Load PTB-XL Metadata (Error Handling Applied)\ntry:\n    df = pd.read_csv(data_path + \"ptbxl_database.csv\", encoding='utf-8')\nexcept Exception as e:\n    print(\"Error loading ptbxl_database.csv:\", e)\n\n\n# Drop rows where diagnostic_class is missing (important for classification)\nscp_df_clean = scp_df.dropna(subset=[\"diagnostic_class\"]).copy()  # Explicit Copy\n\n# Fill missing values for subclass and category labels\nscp_df_clean.loc[:, \"diagnostic_subclass\"] = scp_df_clean[\"diagnostic_subclass\"].fillna(\"Unknown\")\n\n# Drop columns with >70% missing data\ndrop_cols = [\"AHA code\", \"aECG REFID\", \"CDISC Code\", \"DICOM Code\", \"form\", \"rhythm\"]\nscp_df_clean = scp_df_clean.drop(columns=drop_cols)  # No `inplace=True`\n\n# Display cleaned dataset info\nprint(\"Cleaned SCP-ECG Statements Data:\\n\", scp_df_clean.info())\n\n# Fill missing numerical values with median\nnum_cols = [\"age\", \"height\", \"weight\"]\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Fill missing categorical values with \"Unknown\"\ncat_cols = [\"sex\", \"device\", \"validated_by\", \"site\"]\ndf[cat_cols] = df[cat_cols].fillna(\"Unknown\")\n\n# Convert recording_date to datetime\ndf[\"recording_date\"] = pd.to_datetime(df[\"recording_date\"], errors=\"coerce\")\n\n# Drop any remaining rows with critical missing data\ndf_clean = df.dropna()\n\n# Display cleaned dataset info\nprint(\"Cleaned PTB-XL Database Data:\\n\", df_clean.info())\n\n# Check for any remaining missing values\nprint(\"Remaining Missing Values in SCP:\\n\", scp_df_clean.isnull().sum())\nprint(\"\\nRemaining Missing Values in PTB-XL:\\n\", df_clean.isnull().sum())\n\n# Save cleaned data for faster processing later\nscp_df_clean.to_csv(\"cleaned_scp_statements.csv\", index=False)\ndf_clean.to_csv(\"cleaned_ptbxl_database.csv\", index=False)\n\nprint(\"Cleaned datasets saved successfully!\")\n\n# Add full file paths for high-resolution (500 Hz) and low-resolution (100 Hz) ECG files\ndf[\"file_path_500\"] = data_path + df[\"filename_hr\"]  # 500Hz file paths\ndf[\"file_path_100\"] = data_path + df[\"filename_lr\"]  # 100Hz file paths\n\n# Confirm that the columns are now present\nprint(\"Available Columns in DataFrame After Fix:\\n\", df.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ast  # To parse scp_codes from string to dictionary\nimport wfdb\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# ✅ Load cleaned metadata\ndf_metadata = pd.read_csv(\"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/ptbxl_database.csv\")\n\n# ✅ Convert 'scp_codes' column from string to dictionary\ndf_metadata[\"scp_codes\"] = df_metadata[\"scp_codes\"].apply(ast.literal_eval)\n\n# ✅ Define MI and STTC labels\nmi_labels = ['INJAS', 'ILMI', 'INJIL', 'PMI', 'ISCAL', 'ISCLA', 'AMI', 'LMI', 'INJAL', \n             'ALMI', 'INJIN', 'IPMI', 'ISC_', 'IMI', 'ISCAS', 'ISCIN', 'INJLA', 'ISCIL', \n             'ISCAN', 'IPLMI', 'ASMI']\nsttc_labels = ['NST_', 'STE_', 'STD_', 'STACH']\n\n# ✅ Function to assign diagnostic class\ndef assign_diagnostic_class(scp_dict):\n    scp_keys = list(scp_dict.keys())\n    if any(label in scp_keys for label in mi_labels):\n        return \"MI\"\n    elif any(label in scp_keys for label in sttc_labels):\n        return \"STTC\"\n    elif \"NORM\" in scp_keys:\n        return \"NORM\"\n    return \"OTHER\"\n\n# ✅ Apply function to dataset\ndf_metadata[\"diagnostic_class\"] = df_metadata[\"scp_codes\"].apply(assign_diagnostic_class)\n\n# ✅ Filter out OTHER\ndf_metadata = df_metadata[df_metadata[\"diagnostic_class\"] != \"OTHER\"]\n\n# ✅ Reassign MI and STTC into ABNORM for binary classification\ndf_metadata[\"diagnostic_class\"] = df_metadata[\"diagnostic_class\"].apply(lambda x: \"NORM\" if x == \"NORM\" else \"ABN\")\n\n# ✅ Encode binary labels: NORM = 0, ABNORM = 1\nlabel_mapping = {\"NORM\": 0, \"ABN\": 1}\ndf_metadata[\"label\"] = df_metadata[\"diagnostic_class\"].map(label_mapping)\n\n# ✅ Print updated label distribution\nprint(\"\\n✅ Final Binary Label Distribution:\")\nprint(df_metadata[\"diagnostic_class\"].value_counts())\n\n# ✅ Display sample metadata\nprint(\"\\n✅ Sample Metadata with Labels:\")\nprint(df_metadata[[\"scp_codes\", \"diagnostic_class\", \"label\"]].head())\n\n# ✅ Define file paths (ensure data_path is defined)\ndata_path = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/\"\ndf_metadata[\"file_path_100\"] = data_path + df_metadata[\"filename_lr\"]\ndf_metadata[\"file_path_500\"] = data_path + df_metadata[\"filename_hr\"]\n\n# ✅ Debug: Check columns\nprint(\"\\n✅ Columns in df_metadata:\", df_metadata.columns)\n\n# ✅ Sample data preview\nprint(df_metadata[[\"file_path_100\", \"file_path_500\", \"label\"]].head())\n\n# ✅ Function to load ECG signal\ndef load_ecg(record_path):\n    record = wfdb.rdrecord(record_path)\n    return np.array(record.p_signal)  # ECG signal as NumPy array\n\n# ✅ Downsample Normal class (if needed) to balance dataset\nnorm_samples = df_metadata[df_metadata[\"diagnostic_class\"] == \"NORM\"]\nabnorm_samples = df_metadata[df_metadata[\"diagnostic_class\"] == \"ABN\"]\ndf_balanced = pd.concat([norm_samples, abnorm_samples])\n\n# ✅ Shuffle\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# ✅ New label distribution\nprint(\"✅ New Dataset Size:\")\nprint(df_balanced[\"diagnostic_class\"].value_counts())\n\n# ✅ Load ECG signals (100Hz only for memory efficiency)\nsubset_size = len(df_balanced)\necg_100Hz = np.array([load_ecg(f) for f in tqdm(df_balanced[\"file_path_100\"][:subset_size])])\n\n# ✅ ECG data shape\nprint(\"✅ Loaded ECG Subset Shape (100Hz):\", ecg_100Hz.shape)\n\n# ✅ Final label distribution check\nprint(\"✅ Label Distribution in Processed Data:\")\nprint(df_balanced[\"diagnostic_class\"].value_counts())\n\n# ✅ Debug: Preview of file paths and labels\nprint(df_balanced[[\"file_path_100\", \"file_path_500\", \"diagnostic_class\", \"label\"]].head())\n\n# ✅ Plot a random ECG signal (first lead)\nsample_idx = np.random.randint(0, ecg_100Hz.shape[0])\nplt.figure(figsize=(12, 4))\nplt.plot(ecg_100Hz[sample_idx, 0, :], label=\"Lead 1 ECG Signal (100Hz)\", linestyle=\"dashed\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Amplitude (µV)\")\nplt.title(f\"ECG Signal from Record {sample_idx}\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# ✅ Prepare features (X) and labels (y)\nX = ecg_100Hz  # Shape: (samples, leads, time)\ny = df_balanced[\"label\"].values  # 0 = NORM, 1 = ABNORM\n\n# ✅ Identify normal (healthy) indices\nhealthy_indices = np.where(y == 0)[0]\nabnorm_indices = np.where(y == 1)[0]\n\n# ✅ Split healthy data for training the autoencoder\nx_healthy = X[healthy_indices]\nx_healthy_train, x_healthy_test = train_test_split(x_healthy, test_size=0.2, random_state=42)\n\n# ✅ Split entire dataset for evaluation\nx_all_train, x_all_test, y_all_train, y_all_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# ✅ Final checks\nprint(\"\\n✅ Autoencoder Training Data (Healthy Only):\")\nprint(\"Train:\", x_healthy_train.shape, \" Test:\", x_healthy_test.shape)\n\nprint(\"\\n✅ Evaluation Dataset (All Data):\")\nprint(\"Train:\", x_all_train.shape, \" Test:\", x_all_test.shape)\nprint(\"Train labels:\", np.bincount(y_all_train))\nprint(\"Test labels:\", np.bincount(y_all_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Count normal (0) and anomalous (1) labels\nprint(\"✅ Test Label Distribution:\")\nprint(\"Normal     (0):\", np.sum(y_all_test == 0))\nprint(\"Anomalous  (1):\", np.sum(y_all_test == 1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# ✅ Update label titles for binary classification\nlabel_title = [\"NORM\", \"ABN\"]\n\n# ✅ Function to plot ECG samples\ndef plot_ecg_samples(X_data, y_data, dataset_name):\n    unique_labels = np.unique(y_data)\n    fig, axes = plt.subplots(len(unique_labels), 1, figsize=(12, 6), sharex=True)\n\n    for i, label in enumerate(unique_labels):\n        # Find the first sample for each class\n        sample_idx = np.where(y_data == label)[0][0]\n        \n        # Plot only Lead I (first channel)\n        axes[i].plot(X_data[sample_idx, :, 0], label=f\"Label {label} - {label_title[label]}\")\n        axes[i].set_title(f\"{dataset_name} - Class: {label_title[label]}\")\n        axes[i].legend()\n        axes[i].grid(True)\n\n    plt.xlabel(\"Time Steps\")\n    plt.tight_layout()\n    plt.show()\n\n# ✅ Show samples from your full classification dataset\nplot_ecg_samples(x_all_train, y_all_train, \"Training (Binary Labels)\")\nplot_ecg_samples(x_all_test, y_all_test, \"Testing (Binary Labels)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.layers import (\n#     Input, Conv1D, BatchNormalization, Dropout, LSTM, Dense, Flatten,\n#     Conv1DTranspose, Reshape, TimeDistributed, RepeatVector\n# )\n\n# # ✅ Define input shape from healthy training data\n# input_shape = (x_healthy_train.shape[1], x_healthy_train.shape[2])  # e.g., (1000, 12)\n\n# # ✅ Input Layer\n# input_layer = Input(shape=input_shape)\n\n# # ✅ CNN Encoder\n# x = Conv1D(128, kernel_size=7, strides=1, activation=\"relu\", padding=\"same\")(input_layer)\n# x = BatchNormalization()(x)\n# x = Dropout(0.3)(x)\n\n# x = Conv1D(256, kernel_size=5, strides=1, activation=\"relu\", padding=\"same\")(x)\n# x = BatchNormalization()(x)\n# x = Dropout(0.3)(x)\n\n# x = Flatten()(x)\n# encoded = Dense(256, activation=\"relu\")(x)\n\n# # ✅ LSTM Decoder\n# x = RepeatVector(input_shape[0])(encoded)\n# x = LSTM(256, return_sequences=True)(x)\n# x = LSTM(128, return_sequences=True)(x)\n# x = LSTM(64, return_sequences=True)(x)\n\n# # ✅ CNN Decoder\n# x = Conv1DTranspose(128, kernel_size=5, strides=1, activation=\"relu\", padding=\"same\")(x)\n# x = BatchNormalization()(x)\n\n# x = Conv1DTranspose(64, kernel_size=7, strides=1, activation=\"relu\", padding=\"same\")(x)\n# x = BatchNormalization()(x)\n\n# # ✅ Output layer with sigmoid activation (normalized output)\n# decoded = Conv1DTranspose(input_shape[1], kernel_size=7, strides=1, activation=\"sigmoid\", padding=\"same\")(x)\n\n# # ✅ Compile Autoencoder\n# autoencoder = Model(inputs=input_layer, outputs=decoded)\n# autoencoder.compile(optimizer=\"adam\", loss=\"mae\")\n\n# # ✅ Summary\n# autoencoder.summary()\n\n# # ✅ Early Stopping Callback\n# early_stopping = tf.keras.callbacks.EarlyStopping(\n#     monitor=\"val_loss\",\n#     patience=10,\n#     restore_best_weights=True\n# )\n\n# # ✅ Train Autoencoder on healthy ECGs only\n# history = autoencoder.fit(\n#     x_healthy_train, x_healthy_train,  # Self-reconstruction\n#     validation_data=(x_healthy_test, x_healthy_test),\n#     epochs=1000,\n#     batch_size=64,\n#     callbacks=[early_stopping],\n#     verbose=1\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.layers import (\n#     Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization,\n#     Bidirectional, LSTM, RepeatVector, TimeDistributed, Dense, UpSampling1D\n# )\n\n# # ✅ Input shape (e.g., 1000 timesteps, 12 leads)\n# input_shape = (x_healthy_train.shape[1], x_healthy_train.shape[2])  # (1000, 12)\n\n# # ✅ Encoder\n# inputs = Input(shape=input_shape)\n\n# x = Conv1D(64, kernel_size=7, activation=\"relu\", padding=\"same\")(inputs)\n# x = MaxPooling1D(pool_size=2)(x)\n# x = BatchNormalization()(x)\n\n# x = Conv1D(128, kernel_size=5, activation=\"relu\", padding=\"same\")(x)\n# x = MaxPooling1D(pool_size=2)(x)\n# x = BatchNormalization()(x)\n# x = Dropout(0.3)(x)\n\n# # ✅ Bottleneck: BiLSTM\n# x = Bidirectional(LSTM(64, return_sequences=False))(x)\n# encoded = Dense(128, activation=\"relu\")(x)\n\n# # ✅ Decoder\n# x = RepeatVector(input_shape[0] // 4)(encoded)\n# x = LSTM(64, return_sequences=True)(x)\n# x = UpSampling1D(size=2)(x)\n\n# x = Conv1D(128, kernel_size=5, activation=\"relu\", padding=\"same\")(x)\n# x = BatchNormalization()(x)\n# x = UpSampling1D(size=2)(x)\n\n# x = Conv1D(64, kernel_size=7, activation=\"relu\", padding=\"same\")(x)\n# x = BatchNormalization()(x)\n\n# # ✅ Output Layer\n# decoded = Conv1D(input_shape[1], kernel_size=7, activation=\"sigmoid\", padding=\"same\")(x)\n\n# # ✅ Compile Model\n# autoencoder = Model(inputs, decoded)\n# autoencoder.compile(optimizer=\"adam\", loss=\"mae\")\n# autoencoder.summary()\n\n# # ✅ Early Stopping\n# early_stopping = tf.keras.callbacks.EarlyStopping(\n#     monitor=\"val_loss\",\n#     patience=10,\n#     restore_best_weights=True\n# )\n\n# # ✅ Train\n# history = autoencoder.fit(\n#     x_healthy_train, x_healthy_train,\n#     validation_data=(x_healthy_test, x_healthy_test),\n#     epochs=50,\n#     batch_size=32,\n#     # callbacks=[early_stopping],\n#     verbose=1\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, Dropout, Activation, Reshape, Permute\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, Dropout, Activation, Reshape, Permute\n\n# Input: (1000, 12)\ninputs = Input(shape=(1000, 12))\n\n# Reshape to (12, 1000, 1)\nx = Permute((2, 1))(inputs)          # (1000, 12) → (12, 1000)\nx = Reshape((12, 1000, 1))(x)        # Add channel dim → (12, 1000, 1)\n\n# Encoder\nx = Conv2D(32, (3, 3), strides=(1, 2), padding='same', activation='tanh')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\n\nx = Conv2D(64, (3, 3), strides=(1, 2), padding='same', activation='tanh')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\n\nx = Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='tanh')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\n\n# Decoder\nx = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='tanh')(x)\nx = BatchNormalization()(x)\n\nx = Conv2DTranspose(32, (3, 3), strides=(1, 2), padding='same', activation='tanh')(x)\nx = BatchNormalization()(x)\n\nx = Conv2DTranspose(1, (3, 3), strides=(1, 2), padding='same', activation='tanh')(x)  # back to 12x1000x1\n\n# Output: reshape back to (1000, 12)\nx = Reshape((12, 1000))(x)\ndecoded = Permute((2, 1))(x)  # → (1000, 12)\n\n# Model\nautoencoder = Model(inputs, decoded)\nautoencoder.compile(optimizer='adam', loss='mae')\nautoencoder.summary()\n\n# Normalize input\nx_healthy_train_scaled = x_healthy_train / np.max(np.abs(x_healthy_train), axis=(1,2), keepdims=True)\nx_healthy_test_scaled  = x_healthy_test  / np.max(np.abs(x_healthy_test), axis=(1,2), keepdims=True)\n\n# Train\nautoencoder.fit(\n    x_healthy_train_scaled, x_healthy_train_scaled,\n    validation_data=(x_healthy_test_scaled, x_healthy_test_scaled),\n    epochs=1000,\n    batch_size=32\n)\n\nautoencoder.save('autoencoder_v3.h5')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.losses import MeanAbsoluteError\n\n# Load the saved model (compiled with 'mae' loss)\nautoencoder = load_model(\n    \"/kaggle/input/autoencoder_v3/keras/default/1/autoencoder_v3.h5\",\n    custom_objects={\"mae\": MeanAbsoluteError()}\n)\n\nprint(\"✅ Autoencoder loaded successfully.\")\n\n# ✅ Normalize Test Data\nX_test = np.copy(x_all_test)\nX_test = (X_test - np.min(X_test)) / (np.max(X_test) - np.min(X_test))\n\nprint(f\"Data SHAPE {X_test.shape}\")\n\n# ✅ Predict Reconstructed ECGs\nX_test_reconstructed = autoencoder.predict(X_test)\n\n# ✅ Compute Reconstruction Error (MAE per sample)\nreconstruction_errors = np.mean(np.abs(X_test - X_test_reconstructed), axis=(1, 2))\n\n# ✅ Set Threshold (95th percentile)\nthreshold = np.percentile(reconstruction_errors, 90)\npred_label = (reconstruction_errors > threshold).astype(int)  # 1 = Anomalous, 0 = Normal\n\n# ✅ Print Summary\nprint(f\"Threshold (95th percentile): {threshold}\")\nprint(\"\\n✅ Reconstruction Error Stats:\")\nprint(f\"Mean: {np.mean(reconstruction_errors):.5f} | Std: {np.std(reconstruction_errors):.5f}\")\nprint(\"\\n✅ Anomaly Predictions:\")\nprint(f\"Predicted Normals: {(pred_label == 0).sum()} | Anomalies: {(pred_label == 1).sum()}\")\n\n# ✅ Optional: Compare with ground truth\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"\\n✅ Classification Report (Autoencoder vs. Ground Truth):\")\n# print(classification_report(y_all_test, anomaly_labels, target_names=[\"Normal\", \"Abnormal\"]))\n\n# ✅ Visualize a Normal ECG\nnormal_indices = np.where(pred_label == 0)[0]\nif len(normal_indices) > 0:\n    idx = normal_indices[0]\n    fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n    axes[0].plot(X_test[idx, :, 0], label=\"Original ECG - Normal\")\n    axes[0].set_title(\"Original ECG - Normal\")\n    axes[1].plot(X_test[idx, :, 0], linestyle=\"--\", label=\"Reconstructed ECG - Normal\", color=\"green\")\n    axes[1].set_title(\"Reconstructed ECG - Normal\")\n    plt.xlabel(\"Time Steps\")\n    plt.tight_layout()\n    plt.show()\n\n# ✅ Visualize an Anomalous ECG\nanomalous_indices = np.where(pred_label == 1)[0]\nif len(anomalous_indices) > 0:\n    idx = anomalous_indices[0]\n    fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n    axes[0].plot(X_test[idx, :, 0], label=\"Original ECG - Anomalous\")\n    axes[0].set_title(\"Original ECG - Anomalous\")\n    axes[1].plot(X_test_reconstructed[idx, :, 0], label=\"Reconstructed ECG - Anomalous\", color=\"red\")\n    axes[1].set_title(\"Reconstructed ECG - Anomalous\")\n    plt.xlabel(\"Time Steps\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ✅ Step 1: Compute Reconstruction Error (MSE per sample)\nreconstructed_ecg = autoencoder.predict(x_all_test)\nreconstruction_errors = np.mean(np.power(x_all_test - reconstructed_ecg, 2), axis=(1, 2))\n\n# ✅ Step 2: Apply threshold to get anomaly predictions\nthreshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)\npredicted_labels = (reconstruction_errors > threshold).astype(int)\n\n# ✅ Step 3: Ground Truth Labels (from y_all_test)\ntrue_labels = y_all_test  # Already 0 = NORM, 1 = ABN\n\n# ✅ Step 4: Metrics\nprecision = precision_score(true_labels, predicted_labels)\nrecall = recall_score(true_labels, predicted_labels)\nf1 = f1_score(true_labels, predicted_labels)\nauc = roc_auc_score(true_labels, reconstruction_errors)\n\nprint(f\"✅ Precision: {precision:.4f}\")\nprint(f\"✅ Recall:    {recall:.4f}\")\nprint(f\"✅ F1-Score:  {f1:.4f}\")\nprint(f\"✅ AUC-ROC:   {auc:.4f}\")\n\n# ✅ Step 5: ROC Curve\nfpr, tpr, _ = roc_curve(true_labels, reconstruction_errors)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\", color=\"blue\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Autoencoder-Based Anomaly Detection\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# ✅ Step 6: Confusion Matrix\ncm = confusion_matrix(true_labels, predicted_labels)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n            xticklabels=[\"Normal\", \"Anomalous\"], \n            yticklabels=[\"Normal\", \"Anomalous\"])\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install stable-baselines3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import gym\n# import numpy as np\n# from gym import spaces\n# from stable_baselines3 import PPO\n\n# class ECGAnomalyEnv(gym.Env):\n#     def __init__(self, reconstruction_errors, true_labels):\n#         super(ECGAnomalyEnv, self).__init__()\n\n#         # Save input data (MSE reconstruction errors and labels)\n#         self.reconstruction_errors = reconstruction_errors\n#         self.true_labels = true_labels  # 0 = Normal, 1 = Anomalous\n\n#         # Define state space (Normalized reconstruction error)\n#         self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n\n#         # Define action space (Increase, Decrease, or Maintain threshold)\n#         self.action_space = spaces.Discrete(3)  # [0: Decrease, 1: Maintain, 2: Increase]\n\n#         # Initialize threshold and step index\n#         self.threshold = np.percentile(self.reconstruction_errors, 90)  # Start with high threshold\n#         self.current_step = 0\n\n#     def step(self, action):\n#         \"\"\"\n#         RL Agent takes an action to adjust the anomaly threshold.\n#         \"\"\"\n#         # Adjust threshold based on action\n#         if action == 0:  # Decrease threshold\n#             self.threshold *= 0.95\n#         elif action == 2:  # Increase threshold\n#             self.threshold *= 1.05\n\n#         # Get current sample reconstruction error & ground truth label\n#         recon_error = self.reconstruction_errors[self.current_step]\n#         true_label = self.true_labels[self.current_step]\n\n#         # Apply threshold-based classification\n#         predicted_label = 1 if recon_error > self.threshold else 0\n\n#         # Compute reward\n#         if predicted_label == true_label:  # Correct classification\n#             reward = 1\n#         elif predicted_label == 1 and true_label == 0:  # False Positive (Penalize)\n#             reward = -1\n#         else:  # False Negative (Critical Error)\n#             reward = -2\n\n#         # Move to next step\n#         self.current_step += 1\n#         done = self.current_step >= len(self.reconstruction_errors)  # Episode ends at dataset end\n\n#         # Return new state, reward, and done flag\n#         return np.array([recon_error], dtype=np.float32), reward, done, {}\n\n#     def reset(self):\n#         \"\"\"\n#         Reset the environment at the beginning of each episode.\n#         \"\"\"\n#         self.current_step = 0\n#         self.threshold = np.percentile(self.reconstruction_errors, 90)  # Reset threshold\n#         return np.array([self.reconstruction_errors[self.current_step]], dtype=np.float32)\n\n# import csv\n# import os\n# from stable_baselines3.common.callbacks import BaseCallback\n\n# # ✅ Define Custom Logging Callback to Save Training Logs in CSV\n# class PPOLoggingCallback(BaseCallback):\n#     def __init__(self, log_file=\"ppo_training_logs.csv\", verbose=1):\n#         super(PPOLoggingCallback, self).__init__(verbose)\n#         self.log_file = log_file\n#         self.first_write = not os.path.exists(log_file)  # Check if file exists\n\n#     def _on_step(self) -> bool:\n#         # Collect training data\n#         log_data = {\n#             \"timesteps\": self.num_timesteps,\n#             \"policy_loss\": float(self.model.logger.name_to_value.get(\"train/policy_gradient_loss\", 0)),\n#             \"value_loss\": float(self.model.logger.name_to_value.get(\"train/value_loss\", 0)),\n#             \"explained_variance\": float(self.model.logger.name_to_value.get(\"train/explained_variance\", 0)),\n#             \"entropy_loss\": float(self.model.logger.name_to_value.get(\"train/entropy_loss\", 0)),\n#             \"clip_fraction\": float(self.model.logger.name_to_value.get(\"train/clip_fraction\", 0)),\n#             \"approx_kl\": float(self.model.logger.name_to_value.get(\"train/approx_kl\", 0))\n#         }\n\n#         # Write logs to CSV file\n#         with open(self.log_file, mode=\"a\", newline=\"\") as f:\n#             writer = csv.DictWriter(f, fieldnames=log_data.keys())\n\n#             if self.first_write:\n#                 writer.writeheader()  # Write headers only once\n#                 self.first_write = False\n\n#             writer.writerow(log_data)  # Append new log data\n\n#         return True  # Continue training\n\n# # ✅ Integrate Logging Callback into PPO Training\n# log_callback = PPOLoggingCallback(log_file=\"ppo_training_logs.csv\")\n\n# # ✅ Now logs will be saved in `ppo_training_logs.csv` during training! 🎯\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport os\nimport csv\nimport matplotlib.pyplot as plt\nfrom gym import spaces\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import BaseCallback\n\nclass ECGAnomalyEnv(gym.Env):\n    def __init__(self, reconstruction_errors, true_labels, patient_ages):\n        super(ECGAnomalyEnv, self).__init__()\n        self.reconstruction_errors = reconstruction_errors\n        self.true_labels = true_labels\n        self.patient_ages = patient_ages  # New input for age\n        self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)  # Include reconstruction error and age as state\n        self.action_space = spaces.Discrete(3)\n        self.threshold = np.percentile(self.reconstruction_errors, 90)\n        self.current_step = 0\n        self.logged_thresholds = []\n        self.logged_rewards = []\n\n    def step(self, action):\n        # Adjust threshold based on action\n        if action == 0:\n            self.threshold *= 0.95\n        elif action == 2:\n            self.threshold *= 1.05\n\n        # Get reconstruction error, true label, and patient's age\n        recon_error = self.reconstruction_errors[self.current_step]\n        true_label = self.true_labels[self.current_step]\n        age = self.patient_ages[self.current_step]\n\n        # Predicted label based on reconstruction error\n        predicted_label = 1 if recon_error > self.threshold else 0\n\n        # Reward calculation\n        reward = self.compute_reward(predicted_label, true_label, age)\n\n        # Log the thresholds and rewards\n        self.logged_thresholds.append(self.threshold)\n        self.logged_rewards.append(reward)\n\n        self.current_step += 1\n        done = self.current_step >= len(self.reconstruction_errors)\n        return np.array([recon_error, age], dtype=np.float32), reward, done, {}\n\n    def reset(self):\n        self.current_step = 0\n        self.threshold = np.percentile(self.reconstruction_errors, 90)\n        self.logged_thresholds = []\n        self.logged_rewards = []\n        return np.array([self.reconstruction_errors[self.current_step], self.patient_ages[self.current_step]], dtype=np.float32)\n\n    def compute_reward(self, predicted_label, true_label, age):\n        # Base reward based on label prediction\n        if predicted_label == true_label:\n            reward = 1  # Correct detection\n        elif predicted_label == 1 and true_label == 0:\n            reward = -1  # False positive\n        else:\n            reward = -2  # False negative (critical error)\n\n        # Adjust reward based on age group\n        if 18 <= age <= 35:\n            reward *= 1.2  # Young adults\n        elif 36 <= age <= 60:\n            reward *= 1  # Middle-aged adults\n        else:\n            reward *= 0.8  # Senior adults\n\n        return reward\n\n\n# ✅ 2. PPO Logging Callback for Training Metrics\nclass PPOLoggingCallback(BaseCallback):\n    def __init__(self, log_file=\"ppo_training_logs.csv\", verbose=1):\n        super(PPOLoggingCallback, self).__init__(verbose)\n        self.log_file = log_file\n        self.first_write = not os.path.exists(log_file)\n\n    def _on_step(self) -> bool:\n        log_data = {\n            \"timesteps\": self.num_timesteps,\n            \"policy_loss\": float(self.model.logger.name_to_value.get(\"train/policy_gradient_loss\", 0)),\n            \"value_loss\": float(self.model.logger.name_to_value.get(\"train/value_loss\", 0)),\n            \"explained_variance\": float(self.model.logger.name_to_value.get(\"train/explained_variance\", 0)),\n            \"entropy_loss\": float(self.model.logger.name_to_value.get(\"train/entropy_loss\", 0)),\n            \"clip_fraction\": float(self.model.logger.name_to_value.get(\"train/clip_fraction\", 0)),\n            \"approx_kl\": float(self.model.logger.name_to_value.get(\"train/approx_kl\", 0))\n        }\n        with open(self.log_file, mode=\"a\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=log_data.keys())\n            if self.first_write:\n                writer.writeheader()\n                self.first_write = False\n            writer.writerow(log_data)\n        return True\n\n# ✅ Fixed: Add _on_step method to make the class instantiable\nclass PPOExtendedLoggingCallback(BaseCallback):\n    def __init__(self, env, log_dir=\"ppo_logs\", verbose=1):\n        super(PPOExtendedLoggingCallback, self).__init__(verbose)\n        self.env = env\n        self.log_dir = log_dir\n        os.makedirs(log_dir, exist_ok=True)\n\n    def _on_step(self) -> bool:\n        return True  # Required by BaseCallback\n\n    def _on_training_end(self):\n        np.save(os.path.join(self.log_dir, \"dynamic_thresholds.npy\"), np.array(self.env.logged_thresholds))\n        np.save(os.path.join(self.log_dir, \"reward_progression.npy\"), np.array(self.env.logged_rewards))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# ✅ Use already loaded and processed data\n# X = ecg_100Hz (shape: samples, leads, time)\n# y = df_balanced[\"label\"].values\n\n# ✅ Normalize signals (optional, for better training)\nX_normalized = (ecg_100Hz - np.min(ecg_100Hz)) / (np.max(ecg_100Hz) - np.min(ecg_100Hz))\n\n# ✅ Transpose shape to (samples, time, leads) if needed\nif X_normalized.shape[1] == 12:\n    X_normalized = np.transpose(X_normalized, (0, 2, 1))  # From (N, 12, T) to (N, T, 12)\n\n# ✅ Split test dataset (already done earlier)\n# x_all_test, y_all_test = ...\n\n# ✅ Print info\nprint(\"Test Set Shape:\", x_all_test.shape)\nprint(\"Test Labels Shape:\", y_all_test.shape)\n\n# ✅ Count labels in test set\nprint(\"\\n✅ Test Label Distribution:\")\nprint(\"Normal     (0):\", np.sum(y_all_test == 0))\nprint(\"Anomalous  (1):\", np.sum(y_all_test == 1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# ✅ Normalize all ECG signals (global)\nX_all = (ecg_100Hz - np.min(ecg_100Hz)) / (np.max(ecg_100Hz) - np.min(ecg_100Hz))\n\n# ✅ Reshape to (samples, time, leads) if needed\nif X_all.shape[1] == 12:\n    X_all = np.transpose(X_all, (0, 2, 1))  # From (N, 12, T) to (N, T, 12)\n\n# ✅ Get predictions from autoencoder\nreconstructed_ecg = autoencoder.predict(X_all)\n\n# ✅ Compute reconstruction error (MSE per sample)\nreconstruction_errors = np.mean(np.power(X_all - reconstructed_ecg, 2), axis=(1, 2))\n\n# ✅ Get true labels (0 = normal, 1 = anomalous)\ny_true = df_balanced[\"label\"].values\n\n# ✅ Save to .npy files\nnp.save(\"reconstruction_errors.npy\", reconstruction_errors)\nnp.save(\"true_labels.npy\", y_true)\n\nprint(\"✅ Saved reconstruction_errors.npy and true_labels.npy\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Process\nenv = ECGAnomalyEnv(reconstruction_errors, true_labels)\nppo_model = PPO(\n    \"MlpPolicy\", env, verbose=1, \n    learning_rate=0.0005, \n    gamma=0.99, \n    n_steps=512, \n    batch_size=64, \n    clip_range=0.2,  \n    ent_coef=0.02, \n    vf_coef=0.7, \n    max_grad_norm=0.5\n)\n\n# Train PPO model\nlog_callback = PPOLoggingCallback(\"ppo_training_logs.csv\")\nppo_model.learn(total_timesteps=100000, callback=log_callback)\n\n# Save trained PPO model\nppo_model.save(\"ppo_ecg_anomaly_model\")\n\n# Visualization after training\nthresholds = np.load(\"ppo_logs/dynamic_thresholds.npy\")\nrewards = np.load(\"ppo_logs/reward_progression.npy\")\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, color=\"purple\")\nplt.title(\"Dynamic Threshold Over Time\")\nplt.xlabel(\"Timestep\")\nplt.ylabel(\"Threshold\")\n\nplt.subplot(1, 2, 2)\nplt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'), color=\"orange\")\nplt.title(\"Reward Progression\")\nplt.xlabel(\"Timestep\")\nplt.ylabel(\"Reward\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# ✅ Load PPO training logs\ndf_logs = pd.read_csv(\"ppo_training_logs.csv\")\n\n# ✅ Rename Columns If Needed (Check for mismatches)\ndf_logs.columns = df_logs.columns.str.strip().str.lower()  # Normalize column names\n\n# ✅ Plot Learning Curves\nplt.figure(figsize=(12, 5))\n\n# Policy Loss Curve\nplt.subplot(1, 2, 1)\nplt.plot(df_logs[\"timesteps\"], df_logs[\"policy_loss\"], label=\"Policy Loss\", color=\"blue\")\nplt.xlabel(\"Timesteps\")\nplt.ylabel(\"Loss\")\nplt.title(\"PPO Policy Loss Over Time\")\nplt.legend()\n\n# Value Loss & Explained Variance Curve\nplt.subplot(1, 2, 2)\nplt.plot(df_logs[\"timesteps\"], df_logs[\"value_loss\"], label=\"Value Loss\", color=\"red\")\nplt.plot(df_logs[\"timesteps\"], df_logs[\"explained_variance\"], label=\"Explained Variance\", color=\"green\")\nplt.xlabel(\"Timesteps\")\nplt.ylabel(\"Value\")\nplt.title(\"PPO Value Loss & Explained Variance\")\nplt.legend()\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}